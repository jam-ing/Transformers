{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model classes in ðŸ¤— Transformers are designed to be compatible with native PyTorch and TensorFlow 2 and can be used\n",
    "seamlessly with either. In this quickstart, we will show how to fine-tune (or train from scratch) a model using the\n",
    "standard training tools available in either framework. We will also show how to use our included\n",
    "`Trainer` class which handles much of the complexity of training for you.\n",
    "\n",
    "This guide assume that you are already familiar with loading and use our models for inference; otherwise, see the\n",
    "[task summary](https://huggingface.co/transformers/task_summary.html). We also assume that you are familiar with training deep neural networks in either\n",
    "PyTorch or TF2, and focus specifically on the nuances and tools for training models in ðŸ¤— Transformers.\n",
    "\n",
    "Sections:\n",
    "\n",
    "  - [pytorch](#pytorch)\n",
    "  - [tensorflow](#tensorflow)\n",
    "  - [trainer](#trainer)\n",
    "  - [additional-resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning in native PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model classes in ðŸ¤— Transformers that don't begin with `TF` are [PyTorch Modules](https://pytorch.org/docs/master/generated/torch.nn.Module.html), meaning that you can use them just as you would any\n",
    "model in PyTorch for both inference and optimization.\n",
    "\n",
    "Let's consider the common task of fine-tuning a masked language model like BERT on a sequence classification dataset.\n",
    "When we instantiate a model with `PreTrainedModel.from_pretrained`, the model configuration and\n",
    "pre-trained weights of the specified model are used to initialize the model. The library also includes a number of\n",
    "task-specific final layers or 'heads' whose weights are instantiated randomly when not present in the specified\n",
    "pre-trained model. For example, instantiating a model with\n",
    "`BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)` will create a BERT model instance\n",
    "with encoder weights copied from the `bert-base-uncased` model and a randomly initialized sequence classification\n",
    "head on top of the encoder with an output size of 2. Models are initialized in `eval` mode by default. We can call\n",
    "`model.train()` to put it in train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful because it allows us to make use of the pre-trained BERT encoder and easily train it on whatever\n",
    "sequence classification dataset we choose. We can use any PyTorch optimizer, but our library also provides the\n",
    "`AdamW` optimizer which implements gradient bias correction as well as weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer allows us to apply different hyperpameters for specific parameter groups. For example, we can apply\n",
    "weight decay to all parameters other than bias and layer normalization terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a simple dummy training batch using `PreTrainedTokenizer.__call__`. This returns\n",
    "a `BatchEncoding` instance which prepares everything we might need to pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call a classification model with the `labels` argument, the first returned element is the Cross Entropy loss\n",
    "between the predictions and the passed labels. Having already set up our optimizer, we can then do a backwards pass and\n",
    "update the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just get the logits and calculate the loss yourself. The following is equivalent to the previous\n",
    "example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "labels = torch.tensor([1,0])\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "loss = F.cross_entropy(outputs.logits, labels)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can train on GPU by calling `to('cuda')` on the model and inputs as usual.\n",
    "\n",
    "We also provide a few learning rate scheduling tools. With the following, we can set up a scheduler which warms up for\n",
    "`num_warmup_steps` and then linearly decays to 0 by the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then all we have to do is call `scheduler.step()` after `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highly recommend using `Trainer`, discussed below, which conveniently handles the moving parts\n",
    "of training ðŸ¤— Transformers models with features like mixed precision and easy tensorboard logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might be interested in keeping the weights of the pre-trained encoder frozen and optimizing only the\n",
    "weights of the head layers. To do so, simply set the `requires_grad` attribute to `False` on the encoder\n",
    "parameters, which can be accessed with the `base_model` submodule on any task-specific model in the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensorflow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning in native TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can also be trained natively in TensorFlow 2. Just as with PyTorch, TensorFlow models can be instantiated with\n",
    "`PreTrainedModel.from_pretrained` to load the weights of the encoder from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `tensorflow_datasets` to load in the [MRPC dataset](https://www.tensorflow.org/datasets/catalog/glue#gluemrpc) from GLUE. We can then use our built-in\n",
    "`glue_convert_examples_to_features` to tokenize MRPC and convert it to a\n",
    "TensorFlow `Dataset` object. Note that tokenizers are framework-agnostic, so there is no need to prepend `TF` to\n",
    "the pretrained tokenizer name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, glue_convert_examples_to_features\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data = tfds.load('glue/mrpc')\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be compiled and trained as any Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tight interoperability between TensorFlow and PyTorch models, you can even save the model and then reload it\n",
    "as a PyTorch model (or vice-versa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model.save_pretrained('./my_mrpc_model/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a simple but feature-complete training and evaluation interface through `Trainer`\n",
    "and `TFTrainer`. You can train, fine-tune, and evaluate any ðŸ¤— Transformers model with a wide range\n",
    "of training options and with built-in features like logging, gradient accumulation, and mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simply call `trainer.train()` to train and `trainer.evaluate()` to evaluate. You can use your own module as\n",
    "well, but the first argument returned from `forward` must be the loss which you wish to optimize.\n",
    "\n",
    "`Trainer` uses a built-in default function to collate batches and prepare them to be fed into the\n",
    "model. If needed, you can also use the `data_collator` argument to pass your own collator function which takes in the\n",
    "data in the format provided by your dataset and returns a batch ready to be fed into the model. Note that\n",
    "`TFTrainer` expects the passed datasets to be dataset objects from `tensorflow_datasets`.\n",
    "\n",
    "To calculate additional metrics in addition to the loss, you can also define your own `compute_metrics` function and\n",
    "pass it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can view the results, including any calculated metrics, by launching tensorboard in your specified\n",
    "`logging_dir` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [A lightweight colab demo](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing)\n",
    "  which uses `Trainer` for IMDb sentiment classification.\n",
    "\n",
    "- [ðŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples) including scripts for\n",
    "  training and fine-tuning on GLUE, SQuAD, and several other tasks.\n",
    "\n",
    "- [How to train a language model](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb), a detailed\n",
    "  colab notebook which uses `Trainer` to train a masked language model from scratch on Esperanto.\n",
    "\n",
    "- [ðŸ¤— Transformers Notebooks](https://huggingface.co/transformers/notebooks.html) which contain dozens of example notebooks from the community for\n",
    "  training and using ðŸ¤— Transformers on a variety of tasks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
